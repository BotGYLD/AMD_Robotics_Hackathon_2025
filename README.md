**### Make a fork or copy of this repo and fill in your team submission details! ###**

# AMD_Robotics_Hackathon_2025_[Project Name]

## Team Information

**Team:** *Team 17, Bot GYLD*
Guillaume BONIFAS, Yvic PINEAU, Lionel AKRE, Damien FRECHOU

**Summary:** *Task manipulation with conic objects*

**At the beginning**
<img width="1039" height="410" alt="Capture d&#39;écran 2025-12-14 145417" src="https://github.com/user-attachments/assets/a379ae34-03f5-4d82-98b3-22cda614d3a9" />

**In progress**
<img width="1030" height="409" alt="Capture d&#39;écran 2025-12-14 145546" src="https://github.com/user-attachments/assets/0c9c08c8-afc7-44ac-966d-f478f82d1ac6" />

**At the end**
<img width="925" height="352" alt="Capture d&#39;écran 2025-12-14 145656" src="https://github.com/user-attachments/assets/abe3bcfb-6b81-4b4f-84e2-0a80a1c2ac37" />


## Submission Details

### 1. Mission Description

**Objective** : Our robot has to pick a serie of 2 (green and red) large cones and place each one on the smallest one with the same color. 

**Real world application** : This scenario is used in the rehabilitation of patients who receive a bionic hand: it is therefore a **useful use case for training a humanoid** robot to perform this type of movement. 

### 2. Creativity
- Our approach 
**Like a young child** plays with “russian dolls” - and train himself, our small robot needs to **gradually go through stages as a basis**, before gaining more independence.
  
- Innovation, difficulty 
The cones are :
1) Made of hard, **slippery plastic and conic shape**, which doesn't make the task any easier!
2) Must **fit together precisely with little margin for error, even though the arm is short!**
3) in different colors and positions, and the robot **can't make a potentially vital mistake!**

### 3. Technical implementations
- Teleoperation / Dataset capture
  - We produced a series of imitation learning experiments by placing the cones in different locations, sometimes with little space to pass between two cones : First with 6 cones, then the ambition was reduced to 4 cones as a first phase, just what was necessary.
  - https://drive.google.com/file/d/10u0GT6jUsNpfH0MbuVbu4HLHHCnYA5mR/view?usp=drive_link 

<img width="477" height="606" alt="Capture d&#39;écran 2025-12-14 145819" src="https://github.com/user-attachments/assets/1ac5e0d6-d46e-4590-9849-412a64361516" />

- Training
We used the ACT foundation model with different configurations, e.g., 64 batches - 10,000 epochs - 0.1 dropout - 60 seconds per episode.

- Inference
    - A series of tests were launched with different datasets/training models: the last one yielded the best result with a complete sequence in episode 39 out of 40. 
    - https://drive.google.com/file/d/1zgeC48xGRWiNvaGY7yH1LZTVELtEjvJF/view?usp=drive_link 

<img width="477" height="684" alt="Capture d&#39;écran 2025-12-14 150005" src="https://github.com/user-attachments/assets/c25edb2d-1b31-443b-83a0-50742826735c" />

### 4. Ease of use
- How generalizable is your implementation across tasks or environments?
- Flexibility and adaptability of the solution 
    - This scenario is applicable in many use cases, both in industry pick and place operations, and in humanoid personal services. 
- Types of commands or interfaces needed to control the robot
    - The griper requires sufficient hold : when the cone falls, a process will    need to be added to remove the parts that have fallen. 


## Additional Links


**DATASET Hugging Face**

- Url our dataset 
    https://huggingface.co/datasets/botgyld/record-test-1

- Url model used  
    https://huggingface.co/botgyld/so101_act_record-test-1-2


**BEST performing task**

- Video overview - Episode 39  
    AMD_Robotics_Hackaton_2025_BotGYLD_BestPerformingTask_Video39 
    https://drive.google.com/file/d/1zgeC48xGRWiNvaGY7yH1LZTVELtEjvJF/view?usp=drive_link 

- Hugging Face - Episode 39 
    AMD_Robotics_Hackaton_2025_BotGYLD_2025-12-14_115400_HF_Episode39
    https://drive.google.com/file/d/1jLvMwO7mFtS2uPxQWBcdATM39zq1XQU2/view?usp=drive_link 

- Google Doc Link
  https://docs.google.com/document/d/15y0cBBkBScYKLkJ80m20bMdQ8k-Ns2PPtrgATrLaJTM/edit?tab=t.0
  

**Our team Bot GYLD**
Guillaume Bonifas  - https://www.linkedin.com/in/guillaumebonifas/ 
Yvic Pineau - https://www.linkedin.com/in/yvicpineau/ 
Lionel Akré - https://www.linkedin.com/in/lionel-akre-87674836/ 
Damien Frechou - https://www.linkedin.com/in/damien-frechou/ 


## Code submission

This is the directory tree of this repo, you need to fill in the `mission` directory with your submission details.

```terminal
AMD_Robotics_Hackathon_2025_ProjectTemplate-main/
├── README.md
└── mission
    ├── code
    │   └── <code and script>
    └── wandb
        └── <latest run directory copied from wandb of your training job>
```


The `latest-run` is generated by wandb for your training job. Please copy it into the wandb sub directory of you Hackathon Repo.

The whole dir of `latest-run` will look like below:

```terminal
$ tree outputs/train/smolvla_so101_2cube_30k_steps/wandb/
outputs/train/smolvla_so101_2cube_30k_steps/wandb/
├── debug-internal.log -> run-20251029_063411-tz1cpo59/logs/debug-internal.log
├── debug.log -> run-20251029_063411-tz1cpo59/logs/debug.log
├── latest-run -> run-20251029_063411-tz1cpo59
└── run-20251029_063411-tz1cpo59
    ├── files
    │   ├── config.yaml
    │   ├── output.log
    │   ├── requirements.txt
    │   ├── wandb-metadata.json
    │   └── wandb-summary.json
    ├── logs
    │   ├── debug-core.log -> /dataset/.cache/wandb/logs/core-debug-20251029_063411.log
    │   ├── debug-internal.log
    │   └── debug.log
    ├── run-tz1cpo59.wandb
    └── tmp
        └── code
```

**NOTES**

1. The `latest-run` is the soft link, please make sure to copy the real target directory it linked with all sub dirs and files.
2. Only provide (upload) the wandb of your last success pre-trained model for the Mission.
